---
title: "Googleì˜ ì‹œê°„ ì‹œê³„ì—´ ì˜ˆì¸¡ì„ ìœ„í•œ TimesFM ì†Œê°œ"
description: ""
coverImage: "/assets/img/2024-07-12-TimesFMGooglesFoundationModelForTime-SeriesForecasting_0.png"
date: 2024-07-12 23:30
ogImage:
  url: /assets/img/2024-07-12-TimesFMGooglesFoundationModelForTime-SeriesForecasting_0.png
tag: Tech
originalTitle: "TimesFM: Googleâ€™s Foundation Model For Time-Series Forecasting"
link: "https://medium.com/towards-data-science/timesfm-googles-foundation-model-for-time-series-forecasting-593a332dd08d"
isUpdated: true
---

![image](/assets/img/2024-07-12-TimesFMGooglesFoundationModelForTime-SeriesForecasting_0.png)

ì•ˆë…•í•˜ì„¸ìš”! êµ¬ê¸€ì´ ì‹œê³„ì—´ ì˜ˆì¸¡ìš© ì¬ë‹¨ ëª¨ë¸ ê²½ìŸì— ì°¸ì—¬í–ˆì–´ìš”.

2023ë…„ 8ì›”, ì‹œê³„ì—´ ì»¤ë®¤ë‹ˆí‹°ëŠ” Nixtlaì˜ ì²« ë²ˆì§¸ ì‹œê³„ì—´ ì˜ˆì¸¡ì„ ìœ„í•œ ì¬ë‹¨ ëª¨ë¸ì¸ TimeGPT ì¶œì‹œë¡œ í˜¼ë€ìŠ¤ëŸ¬ì›Œì¡Œì–´ìš”.

TimeGPTë¥¼ ë”°ë¥´ë©°, ì—¬ëŸ¬ ì¬ë‹¨ ì˜ˆì¸¡ ëª¨ë¸ì´ ì¶œì‹œë˜ì—ˆì§€ë§Œ, í•˜ë‚˜ëŠ” ëˆˆì— ë„ì—ˆì–´ìš”. ìµœê·¼ì— êµ¬ê¸€ì€ í˜„ê²©í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ëŠ” í˜ì‹ ì ì¸ ì‹œê³„ì—´ ëª¨ë¸ì¸ TimesFMì„ ê³µê°œí–ˆì–´ìš”.

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

To be able to forecast accurately, a firm understanding of time series is crucial. It is a versatile tool used in various fields such as retail, energy demand, economics, healthcare, and more. A solid foundation in time series modeling can significantly improve predictive accuracy, just like how GPT-4 revolutionized text prediction.

In this blog post, we will delve into the following topics:

- The unique challenges faced by foundational models in time series compared to NLP (Natural Language Processing).
- How TimesFM tackles these obstacles effectively.
- How TimesFM operates and why it stands out as a robust model.
- Results of benchmark tests conducted on TimesFM.
- The promising future of foundational models in the realm of time-series forecasting.

Excited to explore further? Let's dive in!

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

# Foundation Models for Time Series: Why They Pose a Challenge

The emergence of foundation models has been a significant development in NLP, exemplified by the introduction of GPT-2 in the pivotal work "Language Models are Unsupervised Multitask Learners." However, when it comes to time series data, the task of constructing a foundation model is far from simple. Here are some of the key challenges:

- **Dataset Scarcity:** While text data is abundant in NLP, time-series datasets are not as readily accessible.
- **Unpredictable Format:** Unlike language models that operate on structured grammars and vocabularies, time-series data can vary significantly in format, spanning from sparse sales figures to volatile financial data.
- **Different Granularities:** Time series models are typically designed to work at specific granularities (e.g., hourly, weekly, monthly). Adapting a foundation model to perform effectively across all granularities presents a notable challenge.
- **Variable Contexts and Horizons:** NLP models are optimized for predicting the next word within a defined context length. Conversely, a universal time-series model must contend with variable context lengths and prediction horizons.

Exploring the realms of time series modeling unveils a rich tapestry of complexities and intricacies awaiting solutions.

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

# TimesFMë€ ë¬´ì—‡ì¸ê°€ìš”?

TimesFMì€ ë” êµ¬ì²´ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- 200Mê°œì˜ íŒŒë¼ë¯¸í„°ê°€ ìˆìŠµë‹ˆë‹¤.
- 1000ì–µ ê°œì˜ ì‹¤ì œ ì‹œê°„ ì§€ì ì—ì„œ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤.
- ì¶”ê°€ ê³µë³€ëŸ‰ì„ íŠ¹ì§•ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì¸ê³¼ì  ì…€í”„ ì–´í…ì…˜ê³¼ ì”ì—¬ ë¸”ë¡ì„ í™œìš©í•©ë‹ˆë‹¤.
- ë‹¤ë¥¸ ìµœì²¨ë‹¨ ëª¨ë¸ë“¤ë³´ë‹¤ ì œë¡œìƒ· ì˜ˆì¸¡ì—ì„œ ìš°ìˆ˜í•œ ì„±ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

ë‹¤ìŒìœ¼ë¡œ, TimesFMì´ ì‹œê³„ì—´ ëª¨ë¸ì˜ ê¸°ì´ˆë¥¼ êµ¬ì¶•í•˜ëŠ” ê³¼ì œë¥¼ ì–´ë–»ê²Œ ê·¹ë³µí•˜ëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

# A) ê±°ëŒ€í•œ ì–‘ì˜ ë°ì´í„° ì°¾ê¸°

ê³µê°œ ì‹œê³„ì—´ ë°ì´í„°ì…‹ì€ ë“œë¬¼ë‹¤.

Monash ë¦¬í¬ì§€í† ë¦¬ì™€ ê°™ì€ í‘œì¤€ ë²¤ì¹˜ë§ˆí¬(ì˜ˆ: ê´€ê´‘, ì „ë ¥ ë°ì´í„° ë“±ì„ í¬í•¨í•˜ê³  ìˆìŒ)ë¡œë§Œìœ¼ë¡œëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.

ë³´í¸ì  ì‹œê³„ì—´ ëª¨ë¸ì„ ìœ„í•œ ì´ìƒì ì¸ ë°ì´í„°ì…‹ì€ ì•„ë˜ì™€ ê°™ì€ íŠ¹ì§•ì„ ê°–ì¶°ì•¼ í•©ë‹ˆë‹¤:

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

- ìˆ˜ë°±ë§Œ ê°œì˜ ë°ì´í„°ê°€ í¬í•¨ëœ ëŒ€ê·œëª¨ ë°ì´í„°
- ì—¬ëŸ¬ ë„ë©”ì¸ì„ ëŒ€í‘œí•˜ëŠ” ë‹¤ì–‘í•œ ë°ì´í„°
- ë§¤ë²ˆ ì‹œê°„ ê°„ê²©(ì¼ì¼, ì£¼ê°„ ë“±)

TimesFMì˜ ì €ìë“¤ì€ ê³µê°œ ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ 3ê°€ì§€ ì¶”ê°€ ìì›ì„ ìœ ë¦¬í•˜ê²Œ í™œìš©í–ˆìŠµë‹ˆë‹¤:

- Google Trends: ì €ìë“¤ì€ Google Trends ê²€ìƒ‰ ê´€ì‹¬ì„ ì‹œê°„ë³„ ì‹œí€€ìŠ¤ë¡œ ì¬í™œìš©í–ˆìŠµë‹ˆë‹¤.
- Wiki Pageviews: ì´ë“¤ì€ ëª¨ë“  ìœ„í‚¤ë¯¸ë””ì–´ í˜ì´ì§€ì˜ ì‹œê°„ë³„ ì¡°íšŒìˆ˜ë¥¼ ìº¡ì²˜í–ˆìŠµë‹ˆë‹¤.
- í•©ì„± ë°ì´í„°: ì €ìë“¤ì€ ARMA í”„ë¡œì„¸ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³„ì ˆì„±, ì£¼íŒŒìˆ˜ ë° íŠ¸ë Œë“œê°€ í˜¼í•©ëœ ì‹œê³„ì—´ ë°ì´í„° ì½”í¼ìŠ¤ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.

ê²°ê³¼ëŠ” 1000ì–µ ë°ì´í„°í¬ì¸íŠ¸ë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„°ì…‹ì´ì—ˆìŠµë‹ˆë‹¤.

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

100B ë°ì´í„° í¬ì¸íŠ¸ë„ LLaMaì˜ 1ì¡° í† í°ì— ë¹„í•˜ë©´ ì‘ì§€ë§Œ, ëŒ€ê·œëª¨ ì‹œê³„ì—´ ë°ì´í„°ì…‹ì„ ë§Œë“œëŠ” ì¢‹ì€ ì‹œì‘ì…ë‹ˆë‹¤.

# B) ë³€ìˆ˜ ì»¨í…ìŠ¤íŠ¸ì™€ ì‹œì•¼

ì§€ê¸ˆê¹Œì§€ ì—¬ëŸ¬ ë„ë©”ì¸ê³¼ ì‹œê°„ ë‹¨ìœ„ì—ì„œ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•˜ê³  ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ëŒ€ê·œëª¨ íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ë²”ìš© ì‹œê°„ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ì‹œì•¼ì— ëŒ€í•´ ì–´ë–¤ ê°€ì •ì„ í•´ì•¼ í• ê¹Œìš”?

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

ìˆ˜í‰ ê¸¸ì´ì˜ ì—°êµ¬ (ì˜ˆì¸¡ ê¸¸ì´)
LLMsëŠ” ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë° ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤ (ìê¸°íšŒê·€).

ì‹œê³„ì—´ ëª¨ë¸ì— ëŒ€í•´ ë‘ ê°€ì§€ ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤:

- [3] [4]ì—ì„œì˜ ì—°êµ¬ì— ë”°ë¥´ë©´ ì¥ê¸° ì˜ˆì¸¡ì—ì„œëŠ” ë‹¤ì¤‘ë‹¨ê³„ ìê¸°íšŒê·€ ì ‘ê·¼ ëŒ€ì‹  ì™„ì „í•œ ìˆ˜í‰ì„ ì§ì ‘ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë³´ì¸ë‹¤ê³  í•©ë‹ˆë‹¤.
- ê·¸ëŸ¬ë‚˜ ìš°ë¦¬ì˜ ê²½ìš°(ê³ ì • í–¥ìƒë„ ì˜ˆì¸¡)ì—ëŠ” ìˆ˜í‰ ê¸¸ì´ê°€ ì‚¬ì „ì— ì•Œë ¤ì ¸ ìˆì§€ ì•Šì€ ê²½ìš° ê°€ëŠ¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë²”ìš© ëª¨ë¸ì€ ëª¨ë“  ìˆ˜í‰ ê¸¸ì´ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

TimesFMì€ ë‹¤ë¥¸ ì„±ê³µì ì¸ ëª¨ë¸ì¸ PatchTSTì— ì˜í•´ ì¸ê¸° ìˆëŠ” ê¸°ìˆ ì¸ íŒ¨ì¹­ì„ í™œìš©í•˜ì—¬ ì¤‘ê°„ ì§€ì ì„ ì°¾ìŠµë‹ˆë‹¤.

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

**íŒ¨ì¹­ì˜ ì‘ë™ ë°©ì‹**

TimesFMì€ í•œ ë²ˆì— í•˜ë‚˜ì˜ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ì˜ˆì¸¡í•˜ì§€ ì•Šìœ¼ë©° ì „ì²´ ìˆ˜í‰ ê¸¸ì´ë¥¼ ì˜ˆì¸¡í•˜ì§€ë„ ì•ŠìŠµë‹ˆë‹¤. ëŒ€ì‹ , ì»¨í…ìŠ¤íŠ¸ì™€ ìˆ˜í‰ ê¸¸ì´ ëª¨ë‘ë¥¼ íŒ¨ì¹˜ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.

ë§Œì•½ ìš°ë¦¬ê°€ ë‹¤ìŒê³¼ ê°™ì€ ê²ƒì„ ê°–ê³  ìˆë‹¤ê³  ê°€ì •í•´ ë´…ì‹œë‹¤:

- í¬ê¸° Lì˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´,
- ê·¸ë¦¬ê³  í¬ê¸° pì˜ íŒ¨ì¹˜,
- ê·¸ëŸ¬ë©´ ìš°ë¦¬ëŠ” ì…ë ¥ì„ N = L/p ê°œì˜ íŒ¨ì¹˜ë¡œ ë¶„í• í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²ƒë“¤ì„ ì…ë ¥ íŒ¨ì¹˜ë¼ê³  í•©ë‹ˆë‹¤.
- ë˜í•œ ìˆ˜í‰ ê¸¸ì´ê°€ hì¸ ì¶œë ¥ íŒ¨ì¹˜ê°€ ìˆìŠµë‹ˆë‹¤.
- ì¶œë ¥ íŒ¨ì¹˜ í¬ê¸° ` ì…ë ¥ íŒ¨ì¹˜ í¬ê¸°ë¥¼ í—ˆìš©í•¨ìœ¼ë¡œì¨, ì €ìë“¤ì€ TimesFMì´ ì–´ë– í•œ ê¸¸ì´ì˜ ìˆ˜í‰ì„ ë” ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•  ìˆ˜ ìˆê²Œ ë  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.

Figure 1ì€ TimesFM ì•„í‚¤í…ì²˜ê°€ êµìœ¡ ì¤‘ì— ì–´ë–»ê²Œ ë³´ì´ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

**ì´ë¯¸ì§€ ì²¨ë¶€**

ë¨¼ì €, input_patch_sizeì— ëŒ€í•œ ê°’ì„ pë¡œ ì„ íƒí•˜ê³ , output_patch_sizeì— ëŒ€í•œ ê°’ì„ hë¡œ ì„ íƒí•˜ì„¸ìš”. ëª¨ë¸ì€ ê·¸ëŸ° ë‹¤ìŒ ì…ë ¥ì„ N = L/p ê°œì˜ ì…ë ¥ íŒ¨ì¹˜ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.

ì²« ë²ˆì§¸ í›ˆë ¨ ë³µí˜¸í™” ë‹¨ê³„ì—ì„œ:

- ì²« ë²ˆì§¸ íŒ¨ì¹˜ëŠ” ì…ë ¥ ì”ì°¨ ë¸”ë¡ì—ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤.
- ê²°ê³¼ëŠ” ìœ„ì¹˜ ë¶€í˜¸í™” ë²¡í„°ì— ì¶”ê°€ë©ë‹ˆë‹¤.
- ë‹¨ê³„ 2ì˜ ì¶œë ¥ì€ ìŒ“ì¸ Transformerì— ê³µê¸‰ë©ë‹ˆë‹¤. ê±°ê¸°ì„œ ì¸ê³¼ì  self-attentionì´ ì ìš©ë˜ì–´ ê° ì¶œë ¥ í† í°ì´ ìì‹  ì´ì „ì˜ ì…ë ¥ í† í°ì—ë§Œ ì°¸ì¡°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ë§ˆì°¬ê°€ì§€ë¡œ ë‹¨ê³„ 3ì˜ ì¶œë ¥ì€ ì¶œë ¥ ì”ì°¨ ë¸”ë¡ìœ¼ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ì˜ˆì¸¡ì ì¸ ìˆ˜í‰ì„ ì¸ ì¶œë ¥ íŒ¨ì¹˜ê°€ ìƒì„±ë©ë‹ˆë‹¤. ì´ íŒ¨ì¹˜ëŠ” ì†ì‹¤ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì‹¤ì œ ë°ì´í„°ì™€ ë¹„êµë©ë‹ˆë‹¤.
- ë‹¤ìŒ ë³µí˜¸í™” ë‹¨ê³„ì—ì„œ ëª¨ë¸ì€ ë‹¤ìŒ ë‘ ì…ë ¥ íŒ¨ì¹˜ë¥¼ ì²˜ë¦¬í•˜ê³  ë‘ ë²ˆì§¸ ì¶œë ¥ íŒ¨ì¹˜ë¥¼ ì¶œë ¥í•  ê²ƒì…ë‹ˆë‹¤.

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

ê·¸ëŸ¬ë¯€ë¡œ ê²°ë¡ ì„ ë‚´ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

ì—¬ê¸°ì— ê·¸ê²ƒì´ ìˆìŠµë‹ˆë‹¤! ìš°ë¦¬ëŠ” TimesFMì˜ ì•„í‚¤í…ì²˜ë¥¼ ê°œìš”ë¡œ ì„¤ëª…í–ˆìŠµë‹ˆë‹¤.

## í‰ê°€ ê¸°ì¤€

ë§ˆì§€ë§‰ìœ¼ë¡œ, ì €ìë“¤ì€ TimesFMì„ ë‹¤ë¥¸ SOTA ì˜ˆì¸¡ ëª¨ë¸(í†µê³„ì ì¸, íŠ¸ë¦¬ ê¸°ë°˜, ë”¥ëŸ¬ë‹)ê³¼ ë¹„êµí•˜ì—¬ í‰ê°€í–ˆìŠµë‹ˆë‹¤.

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

Also, the authors compared llmtime with another successful TS foundational model, using GPT-3 and LLaMa-2 with tailored modifications for time-series forecasting.

Here are the benchmark parameters:

- Scaled MAE is utilized - each model's MAE is adjusted by the MAE of a basic naive model due to varying dataset scales.
- All models undergo evaluation on a separate hold-out dataset.
- TimesFM is zero-shot in this comparison - it was not pre-trained on the held-out data.
- In contrast, all other models were explicitly trained and fine-tuned using the held-out data.
- The hold-out dataset comprises time series from three datasets: The Monash Archive, the Darts data benchmark, and the Informer benchmark. The same datasets were used to assess llmtime.

The evaluation results are displayed below:

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

**ì´ë¯¸ì§€ ì¶”ê°€:**

- [TimesFMGooglesFoundationModelForTime-SeriesForecasting_2.png](/assets/img/2024-07-12-TimesFMGooglesFoundationModelForTime-SeriesForecasting_2.png)
- [TimesFMGooglesFoundationModelForTime-SeriesForecasting_3.png](/assets/img/2024-07-12-TimesFMGooglesFoundationModelForTime-SeriesForecasting_3.png)

**ê²°ê³¼ì— ëŒ€í•œ ì°¸ê³ ì‚¬í•­:**

ì—¬ê¸°ì„œ TimesFMì´ ë¶„ëª…í•œ ìŠ¹ìì…ë‹ˆë‹¤.

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

TimesFMì€ ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìƒìœ„ 3ê°œ ëª¨ë¸ ì¤‘ í•˜ë‚˜ë¡œ ì„ ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ llmtimeë„ ëŠ¥ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤.

TimesFMì€ zero-shot ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ëŠ”ë°, ì¦‰ TimesFMì€ í•™ìŠµ ì¤‘ì— í•´ë‹¹ ë°ì´í„°ë¥¼ ì „í˜€ ë³¸ ì ì´ ì—†ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤! ë°˜ë©´ì— ë‹¤ë¥¸ ëª¨ë¸ë“¤ì€ hold-out ì„¸íŠ¸ì˜ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ê°œë³„ì ìœ¼ë¡œ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.

ë˜í•œ, ì„¸ ë²¤ì¹˜ë§ˆí¬ ëª¨ë‘ ë‹¤ì–‘í•œ ë„ë©”ì¸, í¬ê¸°, ì„¸ë¶„ì„± ë° ìˆ˜í‰ ê¸¸ì´ë¥¼ ë‹¤ë£¨ëŠ” ë°ì´í„°ì„¸íŠ¸ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•˜ê³  ë³´ì§€ ëª»í•œ ë°ì´í„°ì— ëŒ€í•´ ê²½ìŸë ¥ ìˆëŠ” ê²°ê³¼ë¥¼ ë‹¬ì„±í•˜ëŠ” ê²ƒì€ ì¸ìƒì ì¸ ì„±ì·¨ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì¶”ì„¸ëŠ” TimeGPT ë²¤ì¹˜ë§ˆí¬ì—ì„œë„ ê´€ì°°ë˜ê³  ìˆìŠµë‹ˆë‹¤.

íŒŒì¸íŠœë‹ì˜ ì˜í–¥

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

TimesFM ë…¼ë¬¸ì€ ì„¸ì„¸í•œ ì„¸ë¶€ì‚¬í•­ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.

ì œë¡œìƒ· ì¶”ë¡ ì€ ê¸°ë³¸ ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” í›Œë¥­í•œ ë°©ë²•ì…ë‹ˆë‹¤. ê·¸ë ‡ì§€ë§Œ í‰ê°€ ë°ì´í„° ì¼ë¶€ì— ëŒ€í•´ TimesFMì„ íŒŒì¸íŠœë‹í–ˆë‹¤ë©´ ê²°ê³¼ëŠ” ì–´ë–»ê²Œ ê°œì„ ë˜ì—ˆì„ê¹Œìš”?

ê²Œë‹¤ê°€, íŒŒì¸íŠœë‹ì€ ìì—°ì–´ ì²˜ë¦¬ì—ì„œ í‘œì¤€ì ì¸ ì‹¤ì²œ ë°©ë²•ì…ë‹ˆë‹¤. ì¬ë¯¸ìˆê²Œë„ TimesGPTëŠ” ì–´ë– í•œ íŒŒì¸íŠœë‹ í›ˆë ¨ ì„¸ë¶€ì‚¬í•­ë„ ê³µìœ í•˜ì§€ ì•Šì§€ë§Œ, í•´ë‹¹ APIë¥¼ í†µí•´ ì§ì ‘ ë°ì´í„°ì…‹ì„ íŒŒì¸íŠœë‹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# ìŠ¤ì¼€ì¼ë§ ë²•ì¹™

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

TimesFMì€ ë³¸ì§ˆì ìœ¼ë¡œ í° ì‚¬ì „ í›ˆë ¨ëœ ë””ì½”ë” ì „ìš© íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì´ê¸° ë•Œë¬¸ì— ëª¨ë¸ í¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ê²ƒì´ íƒ€ë‹¹í•©ë‹ˆë‹¤. ê°„ë‹¨íˆ ë§í•˜ë©´:

ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì€ LM(ì–¸ì–´ ëª¨ë¸)ì˜ ë§¤ê°œ ë³€ìˆ˜ í¬ê¸°, í† í°(ë°ì´í„°ì…‹ í¬ê¸°), í›ˆë ¨ ì‹œê°„ ë° ì„±ëŠ¥ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì„¤ëª…í•˜ëŠ” ê²½í—˜ì ì¸ ê·œì¹™ì…ë‹ˆë‹¤.

ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì€ ì²˜ìŒì—ëŠ” [5]ì—ì„œ ì†Œê°œë˜ì—ˆì§€ë§Œ ë‚˜ì¤‘ì— [6]ì— ì˜í•´ ë‹¤ë“¬ì–´ì¡Œìœ¼ë©° ì´ë¥¼ ì¹˜ì¹˜ì•¼ ë…¼ë¬¸ì´ë¼ê³ ë„ í•©ë‹ˆë‹¤.

ë‹¹ì—°íˆ TimesFM ì €ìë“¤ì€ ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ì…‹ì„ ë™ì¼í•˜ê²Œ ì‚¬ìš©í•˜ê³  ìœ ì‚¬í•œ íšŸìˆ˜ì˜ ë°˜ë³µê¹Œì§€ ì‹œë„í•˜ì—¬ 17M, 70M ë° 200M ë§¤ê°œ ë³€ìˆ˜ í¬ê¸°ì˜ 3ê°œì˜ TimesFM ëª¨ë¸ì„ í›ˆë ¨í•œ ì´ˆê¸° ìŠ¤ì¼€ì¼ë§ ì—°êµ¬ë¥¼ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

ê²°ê³¼ëŠ” ì•„ë˜ì˜ ì´ë¯¸ì§€ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”:

![Figure 4](/assets/img/2024-07-12-TimesFMGooglesFoundationModelForTime-SeriesForecasting_4.png)

ëª¨ë¸ì´ ë” ë§ì€ ë§¤ê°œë³€ìˆ˜ì™€ í•¨ê»˜ í™•ì¥ë˜ëŠ” ê²ƒì„ ëª…ë°±í•˜ê²Œ ë³´ì—¬ì£¼ë©°, ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì´ ë³€í™˜ì ì˜ˆì¸¡ ëª¨ë¸ì—ë„ ì ìš©ëœë‹¤ëŠ” ê²ƒì„ ì…ì¦í–ˆì–´ìš” ([7] ì°¸ì¡°).

ê²Œë‹¤ê°€ ì €ìë“¤ì€ ë°ì´í„°ì…‹ í¬ê¸°ê°€ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì¡°ì‚¬í–ˆì–´ìš”. ì‘ì€ ë°ì´í„°ì…‹(M4, Electricity, Traffic, and Weather)ì—ì„œë§Œ í›ˆë ¨ì‹œí‚¨ ê²°ê³¼ë¥¼ í° ë°ì´í„°ì…‹(Google Trends ë“±) ë° í•©ì„± ë°ì´í„°ì…‹ê³¼ ë¹„êµí–ˆì–´ìš”. ê²°ê³¼ëŠ” ì•„ë˜ì˜ ì´ë¯¸ì§€ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹µë‹ˆë‹¤:

![Figure 5](/assets/img/2024-07-12-TimesFMGooglesFoundationModelForTime-SeriesForecasting_5.png)

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

ì•ˆë…•í•˜ì„¸ìš”!

ì˜¤ëŠ˜ì€ ë˜ ë‹¤ë¥¸ í¥ë¯¸ë¡œìš´ ì£¼ì œì— ëŒ€í•´ ì´ì•¼ê¸°í•´ë³´ë ¤ê³  í•´ìš”. ì—°êµ¬ì—ì„œ ë” ë§ì€ ì„¸ë¶€ì‚¬í•­ê³¼ í•¨ê»˜ ë‹¤ë¥¸ ê´€ê³„ë“¤ë„ í¬í•¨ë˜ì—ˆë‹¤ë©´ ë” í¥ë¯¸ë¡œì› ì„ í…ë°ìš”. íŠ¹íˆ ì„±ëŠ¥ ëŒ€ ë°ì´í„°ì…‹ í¬ê¸° ë° ì„±ëŠ¥ ëŒ€ í•™ìŠµ ì‹œê°„ ê´€ê³„ê°€ ì¢€ ë” ìì„¸íˆ ë‹¤ë¤„ì¡Œë‹¤ë©´ ì¢‹ê² ë„¤ìš”.

ë§ˆì§€ë§‰ìœ¼ë¡œ, ì €ìë“¤ì€ ìì‹ ë“¤ì˜ êµ¬ì¡°ì  ì„ íƒì´ ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ì¸¡ì •í•˜ê¸° ìœ„í•´ Ablation studyë¥¼ ì‹¤ì‹œí–ˆìŠµë‹ˆë‹¤.

ìƒˆë¡œìš´ ì •ë³´ë“¤ì´ ëŠ˜ì–´ë‚˜ë©´ì„œ ì§€ì‹ì„ ë”ìš± í’ë¶€í•˜ê²Œ ë§Œë“¤ì–´ ê°€ëŠ” ê³¼ì •ì´ ì •ë§ í¥ë¯¸ë¡­ì£ ! í•¨ê»˜ ê³µë¶€í•˜ê³  ì„±ì¥í•˜ëŠ” ê²ƒì€ ëŠ˜ ì¦ê±°ìš´ ì¼ì´ì—ìš”. ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”! ğŸŒŸ

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

They mostly focus on the relationships between the performance of TimesFM and the sizes of the input and output patches.

Remember, the choice of breaking the input into patches and allowing the output patches to be larger is the key design choice of TimesFM. If we didnâ€™t have input patches (e.g., the model consumed the full input in a single step), TimesFM would essentially function as an encoder-decoder model, not decoder-only.

Therefore, the authors measured how the performance and the size of input/output patches are related. The analysis is shown in Figure 6 and Figure 7:

![Figure 6](/assets/img/2024-07-12-TimesFMGooglesFoundationModelForTime-SeriesForecasting_6.png)

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

![TimesFM Model Image](/assets/img/2024-07-12-TimesFMGooglesFoundationModelForTime-SeriesForecasting_7.png)

ì–‘ìª½ ê·¸ë¦¼ ëª¨ë‘ ë” ì‘ì€ ì…ë ¥ íŒ¨ì¹˜ (í•˜ì§€ë§Œ ë§¤ìš° ì‘ì§€ëŠ” ì•ŠìŒ)ì™€ ë” í° ì¶œë ¥ íŒ¨ì¹˜ë¥¼ ì„ íƒí•˜ë„ë¡ ì •ë‹¹í™”í•©ë‹ˆë‹¤ â€” íš¨ìœ¨ì ìœ¼ë¡œ TimesFMì„ ë””ì½”ë” ì „ìš© ëª¨ë¸ë¡œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.

# ë§ˆë¬´ë¦¬ ë§ºìŒ

TimesFMì€ Googleì˜ ë†€ë¼ìš´ ì‹œê³„ì—´ ëª¨ë¸ë¡œì„œ â€” ì‹œê³„ì—´ ê¸°ì´ˆ ëª¨ë¸ ì¹´í…Œê³ ë¦¬ì— ì¤‘ìš”í•œ ì¶”ê°€ì…ë‹ˆë‹¤.

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

í˜„ì¬ ëª¨ë¸ì€ ë¹„ê³µê°œ ë² íƒ€ ìƒíƒœì— ìˆì§€ë§Œ êµ¬ê¸€ì€ ì´ë¥¼ Google Cloud Vertex AIì—ì„œ ì´ìš©í•  ìˆ˜ ìˆë„ë¡ ê³„íš ì¤‘ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ëª¨ë¸ì´ë‚˜ í›ˆë ¨ ë°ì´í„°ì„¸íŠ¸ëŠ” ì•„ì§ ê³µê°œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (êµ¬ê¸€ì€ ëª¨ë¸ì„ ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ê³µê°œí• ì§€ ì•„ì§ ê³ ë¯¼ ì¤‘ì…ë‹ˆë‹¤).

ë‹¤í–‰íˆ TimesFM ë…¼ë¬¸ì€ Nixtlaì˜ TimesGPT ë…¼ë¬¸ë³´ë‹¤ í›¨ì”¬ ì •ë³´ê°€ í’ë¶€í•©ë‹ˆë‹¤. ì–´ë–¤ ê¸°ì´ˆ ì˜ˆì¸¡ ëª¨ë¸ì´ì–´ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•œ ë§ì€ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.

ê·¸ë˜ë„ ëŒ€ì¤‘ì ì¸ ì‹œê³„ì—´ ë°ì´í„°ì„¸íŠ¸ì˜ ë¶€ì¬ë¥¼ ê³ ë ¤í•˜ë©´ TS ê¸°ì´ˆ ëª¨ë¸ì— ëŒ€í•œ ì—°êµ¬ëŠ” ì•„ì§ë„ ë©€ì€ ê¸¸ì„ ê°€ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ 2019ë…„ì— T5ê°€ ì¶œì‹œë˜ì—ˆì„ ë•Œ CommonCrawlì—ì„œ íŒŒìƒëœ 750GBì˜ ì •ì œëœ í…ìŠ¤íŠ¸ ë°ì´í„°ì¸ C4 ë°ì´í„°ì„¸íŠ¸ê°€ ì´ë¯¸ ì‚¬ìš© ê°€ëŠ¥í–ˆì—ˆìŠµë‹ˆë‹¤.

ë‚´ ì˜ê²¬ìœ¼ë¡œëŠ”, ì•ìœ¼ë¡œ TS ê¸°ì´ˆ ëª¨ë¸ì— ëŒ€í•œ ë§‰ëŒ€í•œ ì ì¬ë ¥ì´ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤!

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

# ì½ì–´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤!

- Linkedinì—ì„œ ì €ë¥¼ íŒ”ë¡œìš°í•´ ì£¼ì„¸ìš”!
- AI Horizon Forecast ë‰´ìŠ¤ë ˆí„°ë¥¼ êµ¬ë…í•´ ì£¼ì„¸ìš”!

# ì°¸ê³ ë¬¸í—Œ

[1] Das et al., A decoder-only foundation model for time-series forecasting [2023]

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

[2] RadFord et al., ì–¸ì–´ ëª¨ë¸ì€ ê°ë…ë˜ì§€ ì•Šì€ ë‹¤ì¤‘ ì‘ì—… í•™ìŠµìì…ë‹ˆë‹¤

[3] Zhou et al., Informer: ì¥ê¸° ì‹œí€€ìŠ¤ ì‹œê³„ì—´ ì˜ˆì¸¡ì„ ìœ„í•œ íš¨ìœ¨ì ì¸ íŠ¸ëœìŠ¤í¬ë¨¸ ë„˜ì–´ì„œ

[4] Makridakis et al., í†µê³„, ê¸°ê³„ í•™ìŠµ ë° ì‹¬ì¸µ í•™ìŠµ ì˜ˆì¸¡ ë°©ë²•: ë¹„êµ ë° ì „ë§ (2022ë…„ 8ì›”)

[5] Jared Kaplan et al., ì‹ ê²½ë§ ì–¸ì–´ ëª¨ë¸ì˜ í™•ì¥ ë²•ì¹™ë“¤

<!-- cozy-coder - ìˆ˜í‰ -->

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4877378276818686"
     data-ad-slot="1107185301"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>

<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

**(6) Jordan Hoffmann et al. Training Compute-Optimal Large Language Models**

**(7) Manuel Kunz et al. Deep Learning based Forecasting: a case study from the online fashion industry**
